<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Maren  Mahsereci


  | The Kronecker Product

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>Ω</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2022/kronecker/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://mmahsereci.github.io/">
       <span class="font-weight-bold">Maren</span>   Mahsereci
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              <!-- I changed this -->
              posts
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      





<div class="post">

  <header class="post-header">
    <h1 class="post-title">The Kronecker Product</h1>
    <p class="post-meta">August 5, 2022 • mmahsereci</p>
    <p class="post-tags">
      <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>
      
        ·  
        
        <a href="/blog/tag/machinelearning">
          <i class="fas fa-hashtag fa-sm"></i> machinelearning</a>  
          
      

      
        ·  
        
        <a href="/blog/category/techblog">
          <i class="fas fa-tag fa-sm"></i> techblog</a>  
          
      

    </p>
  </header>

  <article class="post-content">
    <p>The Kronecker product is a tensor product that appears frequently in machine learning
and its <a href="#applications-of-the-kronecker-product">applications</a>.
This post collects some useful properties and some other insights on it.</p>

<blockquote>
  <p>There is a follow-up post on the lesser known symmetric and anti-symmetric 
Kronecker product <a href="/blog/2022/kronecker-sym/">here</a>.</p>
</blockquote>

<p>Let \(A\in\mathbb{R}^{n_1\times n_2}\) and \(B\in\mathbb{R}^{m_1\times m_2}\) be two matrices. 
Then, the Kronecker product, denoted `\(\otimes\)’, of \(A\) and \(B\) is in \(\mathbb{R}^{n_1m_1\times n_2m_2}\) and defined as</p>

\[\begin{equation}
\label{eq:def}
\begin{split}
(A\otimes B)_{(ij), (kl)} = A_{ik} B_{jl},\quad 
&amp;i=1\dots n_1,~ k=1\dots n_2\\
&amp;j=1\dots m_1,~ l=1\dots m_2,
\end{split}
\end{equation}\]

<p>where \((ij)\) and \((kl)\) are the double indices 
\((ij) = m_1 (i-1) + j\) and \((kl)=m_2(k-1) + l\).</p>

<p>An example for \(n_1=n_2=m_2=2\) and \(m_1=3\) is</p>

<p>\(\left[\begin{array}{c c} 
A_{11} &amp; A_{12} \\
A_{21} &amp; A_{22} 
\end{array}\right]
\otimes
\left[\begin{array}{c c} 
B_{11} &amp; B_{12} \\
B_{21} &amp; B_{22} \\
B_{31} &amp; B_{32}
\end{array}\right]
=
\left[\begin{array}{c c | c c} 
A_{11}B_{11} &amp; A_{11}B_{12} &amp; A_{12}B_{11} &amp; A_{12}B_{12}\\
A_{11}B_{21} &amp; A_{11}B_{22} &amp; A_{12}B_{21} &amp; A_{12}B_{22} \\
A_{11}B_{31} &amp; A_{11}B_{32} &amp; A_{12}B_{31} &amp; A_{12}B_{32} \\
\hline
A_{21}B_{11} &amp; A_{21}B_{12} &amp; A_{22}B_{11} &amp; A_{22}B_{12}\\
A_{21}B_{21} &amp; A_{21}B_{22} &amp; A_{22}B_{21} &amp; A_{22}B_{22} \\
A_{21}B_{31} &amp; A_{21}B_{32} &amp; A_{22}B_{31} &amp; A_{22}B_{32} \\
\end{array}\right]\).</p>

<p>We observe that the Kronecker product has block structure</p>

\[A\otimes B = 
\left[\begin{array}{c | c | c | c} 
A_{11} B &amp; A_{12} B &amp;\cdots &amp; A_{1n_2} B\\
\hline
A_{21} B &amp; A_{22} B &amp;\cdots &amp; A_{2n_2} B \\
\hline
\vdots &amp; \vdots &amp;\ddots  &amp; \vdots\\
\hline
A_{n_1 1} B &amp; A_{n_1 2} B &amp;\cdots  &amp; A_{n_1n_2} B
\end{array}\right]\]

<p>with \(n_1\times n_2\) blocks where each block is of size \(m_1\times m_2\) and contains 
the matrix \(B\) multiplied with one of the elements of \(A\). This means, a block-diagonal 
matrix with \(n\) identical blocks \(B\) can be written as \(I\otimes B\), where \(I\in\mathbb{R}^{n\times n}\)
is the identity matrix.</p>

<h2 id="kronecker-algebra">Kronecker algebra</h2>

<p>The Kronecker product has some nice algebraic properties which roughly resemble the ones 
of rank-one matrices. For matrices \(A\), \(B\), \(C\) and \(D\) of appropriate sizes 
and properties it is</p>

\[\begin{alignat}{2}
\label{eq:1}
&amp;\text{transpose}           &amp;(A\otimes B)^{\intercal} &amp;= (A^{\intercal}\otimes B^{\intercal}) \\
\label{eq:2}
&amp;\text{inverse}             &amp;(A\otimes B)^{-1} &amp;= A^{-1}\otimes B^{-1} \\
&amp;\text{factorizing}         &amp;(A\otimes B)(C\otimes D)&amp;= (AC\otimes BD) \\
&amp;\text{distributive left}\qquad\qquad   &amp;(A\otimes B) + (A\otimes C) &amp;= A\otimes (B + C) \\
&amp;\text{distributive right}  &amp;(A\otimes B) + (C\otimes B)&amp;= (A+C)\otimes B\\
\label{eq:6}
&amp;\text{associative}         &amp;(A\otimes B)\otimes C&amp;=  A\otimes (B\otimes C)
\end{alignat}\]

<p>where \(A^{\intercal}\) denotes the transpose and \(A^{-1}\) the inverse (should it exist) 
of the matrix \(A\), and respectively for the other matrices. 
Proof of all these equalities follow straightforwardly from the definition of the 
Kronecker product (Eq. \eqref{eq:def}) but can be found in the <a href="#references">references</a> below.</p>

<p>Eqs. \eqref{eq:1}-\eqref{eq:6} especially hold if \(A\), \(B\), \(C\), \(D\) are scalars or 
vectors (where applicable).</p>

<p>All formulas exploit the factorizing structure of the Kronecker product, which is why one side 
of the equality may be much cheaper to compute than the other. 
For example, the left-hand side of Eq. \eqref{eq:2}, 
requires the inverse of a full \(nm\times nm\) matrix which is \(\mathcal{O}(n^3m^3)\), 
while the right-hand side only requires the inverse of an \(n\times n\) and \(m\times m\) matrix
which is \(\mathcal{O}(n^3 + m^3)\).
Properties like this make the Kronecker product interesting for large scale applications.</p>

<p>In general, the Kronecker product does not commute</p>

\[\begin{equation*}
A\otimes B \neq B\otimes A.
\end{equation*}\]

<p>The rank of \(A\otimes B\) is the product of the ranks of \(A\) and \(B\)</p>

\[\operatorname{rk}[A \otimes B ] = \operatorname{rk}[A] \operatorname{rk}[B].\]

<p>Specifically, if \(A\) and \(B\) are square and have full rank, \(A\otimes B\)
is square and has full rank, too.</p>

<p>For square matrices \(A\in\mathbb{R}^{n\times n}\) and \(B\in\mathbb{R}^{m\times m}\),
the determinant of their Kronecker product is given by the product of powers of the
individual determinants</p>

\[\det(A\otimes B) = \det(A)^{m}\det(B)^n.\]

<p>The trace of the Kronecker product is the product of the individual traces</p>

\[\operatorname{tr}[(A\otimes B)]= \operatorname{tr}[A]\operatorname{tr}[B].\]

<h2 id="vector-multiplication">Vector multiplication</h2>

<p>Define the vectorization operation</p>

\[\overrightarrow{\phantom{X}}:\mathbb{R}^{a\times b}\to \mathbb{R}^{ab},\quad
X\mapsto \overrightarrow{X}\]

<p>as stacking the rows of a matrix into a vector. 
A consequence of Eq. \eqref{eq:def} is that a Kronecker product applied to a vectorized matrix 
\(\overrightarrow{X}\) of appropriate size is the vectorized version of two lower-dimensional, 
cheaper matrix-matrix multiplications</p>

\[\begin{equation}
\label{eq:vec}
(A \otimes B ) \overrightarrow{X} = \overrightarrow{AXB^{\intercal}}.
\end{equation}\]

<p>Eq. \eqref{eq:vec} follows directly from Eq. \eqref{eq:def} and the definition of the 
vectorization operation.
Again, due to the factorization property, the right-hand side amounts to 
multiplying two smaller matrices while the left-hand side amounts to a large matrix-vector
multiplication.</p>

<p>An alternative definition of the vectorization operation is 
stacking the columns of a matrix. We then obtain a similar formula 
\((A \otimes B ) \overrightarrow{X} = \overrightarrow{BXA^{\intercal}}\) where the 
locations of \(A\) and \(B\) are swapped on the right-hand side. 
For the remained of this blog post, we will stick with Eq. \eqref{eq:vec} though.</p>

<h2 id="relation-to-the-frobenius-norm">Relation to the Frobenius norm</h2>

<p>Let \(A\in\mathbb{R}^{n_1\times n_2}\) be a matrix. 
The Frobenius norm \(\|\cdot\|_F: \mathbb{R}^{n_1\times n_2}\to \mathbb{R}_{0,+}\) 
is a matrix norm defined as</p>

\[\begin{equation}
\begin{split}
\|A\|_F^2 
= \operatorname{tr}[A^{\intercal}A]
= \sum_{i=1}^{n_1} \sum_{j=1}^{n_2}A_{ij}^2
&amp;= \|\overrightarrow{A}\|^2\\
&amp;= \overrightarrow{A}^{\intercal}\overrightarrow{A}\\
&amp;= \overrightarrow{A}^{\intercal}(I_{n_1}\otimes I_{n_2})\overrightarrow{A},
\end{split}
\end{equation}\]

<p>where \(I_{n_1}\) and \(I_{n_2}\) are identity matrices of sizes \(n_1\) and \(n_2\)
respectively and \(\|\cdot\|\) is the Euclidean norm.</p>

<p>Let \(W\in\mathbb{R}^{n\times n}\) be a positive definite matrix and 
\(A\in\mathbb{R}^{n\times n}\) be an arbitrary square matrix.
The weighted Frobenius norm is defined as</p>

\[\begin{equation}
\begin{split}
\|A\|_{F, W}^2 
= \|W^{\frac{1}{2}}AW^{\frac{1}{2}}\|_F^2
&amp;= \operatorname{tr}[WA^{\intercal}WA]\\
&amp;= \sum_{i, j, k, l=1}^{n}A_{ji}W_{jk}W_{il}A_{kl}\\
&amp;= \overrightarrow{A}^{\intercal}(W\otimes W)\overrightarrow{A}.
\end{split}
\end{equation}\]

<p>Hence, the weighted square Frobenius norm can be expressed as an inner product weighted
with a positive definite Kronecker matrix.</p>

<p>We can generalize the weighted Frobenius norm to allow two positive definite weight matrices
\(W_1\in\mathbb{R}^{n_1\times n_1}\) and \(W_2\in\mathbb{R}^{n_2\times n_2}\)
and a non-square \(A\in\mathbb{R}^{n_1\times n_2}\) such that</p>

\[\begin{equation*}
\begin{split}
\|A\|_{F, W_1, W_2}^2 
&amp;= \operatorname{tr}[W_2A^{\intercal}W_1A]\\
&amp;= \overrightarrow{A}^{\intercal}(W_1\otimes W_2)\overrightarrow{A}.
\end{split}
\end{equation*}\]

<p>It is mentioned here since used in the <a href="#applications-of-the-kronecker-product">applicattion section</a>.</p>

<h2 id="closest-kronecker-product">Closest Kronecker product</h2>

<p>The solution to the closest Kronecker approximation problem gives some insight into
the factorization structure of the Kronecker product which is why it is mentioned
here specifically.</p>

<p>Suppose we are given a large matrix \(C\in\mathbb{R}^{n_1m_1\times n_2m_2}\). 
The closest Kronecker product \(A^*\otimes B^*\) under the Frobenius norm is given by</p>

\[\begin{equation}
\label{eq:argmin}
A^*, B^* = \operatorname*{arg\,min}_{A, B}\|C-A\otimes B\|_F^2.
\end{equation}\]

<p>There exists a fixed, known permutation 
\(\mathcal{R}: \mathbb{R}^{n_1m_1\times n_2m_2}\to \mathbb{R}^{n_1n_2\times m_1m_2}\)
that vectorizes and stacks blocks of \(C\)
(<a href="#references">[1]</a> Section 6) such that the Kronecker
product can be written as an outer product of the vectorized matrices \(\overrightarrow{A}\)
and \(\overrightarrow{B}\) according to 
\(\mathcal{R}(A\otimes B) = \overrightarrow{A}\overrightarrow{B^{\intercal}}\) .
Thus, Eq. \eqref{eq:argmin} can be re-phrased as a rank-one approximation problem in an 
\(n_1n_2\times m_1m_2\) dimensional space</p>

\[\begin{equation*}
\label{eq:argmin-R}
\overrightarrow{A^*}, \overrightarrow{B^*} = \operatorname*{arg\,min}_{A, B}\|\mathcal{R}(C)-\overrightarrow{A} \overrightarrow{B}\|_F^2.
\end{equation*}\]

<p>This directly follows from the definition of the Kronecker product, 
the definition of \(\mathcal{R}\) and the definition of the Frobenius norm.</p>

<p>This means that the closes Kronecker product to \(C\) 
under the Frobenius norm as in Eq. \eqref{eq:argmin}, is equivalent to a rank-one matrix approximation in a permuted 
space defined by \(\mathcal{R}\). It is straightforward to solve rank-one approximations
using e.g., a singular value decomposition of \(\mathcal{R}(C)\). The resulting vectors 
\(\overrightarrow{A^*}\) and \(\overrightarrow{B^*}\) can then be re-shaped with the inverse
vectorization operation in order to obtain the closest Kronecker product 
\(A^*\otimes B^*\) according to Eq. \eqref{eq:argmin}.</p>

<h2 id="applications-of-the-kronecker-product">Applications of the Kronecker product</h2>

<p>Here are some examples of applications where the Kronecker product naturally shows up.</p>

<h3 id="matrix-normal-distribution">Matrix normal distribution</h3>

<p>The Kronecker product naturally occurs in the vectorized version of the 
<a href="https://en.wikipedia.org/wiki/Matrix_normal_distribution" target="_blank" rel="noopener noreferrer">matrix-normal distribution</a>.</p>

<p>Let \(X\in\mathbb{R}^{n_1\times n_2}\) be a matrix-valued random variable that follows 
a matrix normal distribution with density</p>

\[p(X; M, U, V) = \left((2\pi)^{n_1n_2} \det(V)^{n_1}\det(U)^{n_2}\right)^{-\frac{1}{2}}
\exp{\left(-\frac{1}{2}\operatorname{tr}[V^{-1}(X-M)^{\intercal}U^{-1}(X-M)]\right)},\]

<p>parametrized by a mean matrix \(M\in\mathbb{R}^{n_1\times n_2}\) 
and two positive definite matrices \(U\in\mathbb{R}^{n_1\times n_1}\) and 
\(V\in\mathbb{R}^{n_2\times n_2}\).
Then, its vectorized version follows a 
<a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution" target="_blank" rel="noopener noreferrer">multi-variate normal distribution</a>
with vectorized mean matrix \(\overrightarrow{M}\) and 
Kronecker covariance \(U\otimes V\)</p>

\[p(\overrightarrow{X};\overrightarrow{M}, U\otimes V).\]

<p>We can see this equivalence by observing that \(\operatorname{tr}[V^{-1}(X-M)^{\intercal}U^{-1}(X-M)]\)
is linked to the square of the weighted Frobenius norm \(\|X-M\|_{F, U^{-1}, V^{-1}}^2\) and using the factorized 
version of the determinant of \(U\otimes V\).</p>

<p>In this vectorized form, it is also straightforward to analyze the covariance of the
matrix-normal distribution. Since</p>

\[\operatorname{Cov}{[X_{ij}, X_{kl}]} = (U\otimes V)_{(ij), (kl)} = U_{ik}V_{jl}\]

<p>we see that the elements of \(U\) encode how the <em>rows</em> of \(X\) covary, while the elements of 
\(V\) encode how the <em>columns</em> of \(X\) covary. Hence, if e.g., we want to encode independent rows of \(X\),
we may choose \(U = \operatorname{diag}{(u)}\) for some vector \(u\) or even \(U=I\) which yields</p>

\[\operatorname{Cov}{[X, X']} = I\otimes V,\]

<p>a block-diagonal covariance matrix, with \(V\) as blocks.</p>

<p>The <a href="/blog/2022/kronecker-sym/"><em>symmetric</em> Kronecker product</a> 
is further linked to the second moment of the 
<a href="https://en.wikipedia.org/wiki/Wishart_distribution" target="_blank" rel="noopener noreferrer">Wishart distribution</a>
which is a distribution over symmetric positive-definite matrices.</p>

<h3 id="quasi-newton-methods">Quasi-Newton methods</h3>

<p>Quasi-Newton methods (I may make another blogpost on those some day) 
are optimizers for deterministic objective functions
that use a local approximation to the Hessian matrix in order to obtain an approximation
to the Newton direction (this is somewhat a simplification, but the details do not matter here).
Let \(f(w)\) be the objective function to be minimized w.r.t. \(w\) and let \(B_t\)
be the approximation of the Hessian of \(f(w)\) at \(w_t\) and \(B_t^{-1}\) its known inverse.
Then, For some initial point \(w_0\), the update rule of a quasi-Newton method is</p>

\[w_{t+1} = w_t - \alpha_t B_t^{-1} \nabla f(w_t),\]

<p>where \(\alpha_t\) is the step size found by a line search and 
\(B_t\) is the analytic solution to the minimization problem</p>

\[B_t = \operatorname*{arg\,min}_B \|B_{t-1} - B\|_{F, W}^2
\quad\text{ s.t. the secant equation}\quad Bs_t = \Delta y_t\]

<p>with \(s_t = w_t - w_{t-1}\) and \(\Delta y_t = \nabla f(w_t) - \nabla f(w_{t-1})\).
The choice of the symmetric positive definite weight matrix \(W\) leads to the 
different instances of quasi-Newton methods; its precise look is not important 
for the argument here.
We can already see that the vectorized version of the above equation involves 
minimization of a square form, weighted with the Kronecker product \(W \otimes W\)</p>

\[\overrightarrow{B}_t = 
\operatorname*{arg\,min}_{\overrightarrow{B}}
(\overrightarrow{B} - \overrightarrow{B}_{t-1})^{\intercal}
(W\otimes W)
(\overrightarrow{B} - \overrightarrow{B}_{t-1})\]

<p>s.t. the vectorized secant equation that also involves a Kronecker product</p>

\[(I\otimes s_t^{\intercal})\overrightarrow{B} = \Delta y_t.\]

<p>In quasi-Newton methods that enforce symmetry of \(B_t\), we will encounter the 
<a href="/blog/2022/kronecker-sym/"><em>symmetric</em> Kronecker product</a> 
product in a similar way.</p>

<h3 id="linear-algebra">Linear algebra</h3>

<p>Kronecker products also show up in the formulation of solvers for linear systems.
Let \(A\in\mathbb{R}^{n\times n}\) be a matrix and \(b\in\mathbb{R}^{n}\) a vector.
Linear  solvers solve the linear system \(Ax = b\) for the vector \(x\) given the solution 
to matrix-vector multiplications of the form \(A\tilde{s}_t = \Delta \tilde{y}_t\), \(t=0, \dots\). 
Kronecker products show up there naturally, too when vectorizing the equations. 
In fact, there are connections to quasi-Newton methods for certain types of linear systems,
hence the similar algebra.
But this, too, is a topic for another blog post.</p>

<h3 id="deep-learning">Deep learning</h3>

<p>Kronecker products show up in some stochastic optimizers that, similar to quasi-Newton methods
aim to approximate some kind of desired matrix that defines the metric of the space in which
the steepest descent is measured. 
Applying Kronecker products in that context is interesting since i) several quantities 
lend themselves to block structures or block-wise independence assumption due to the
neural network architecture, and
ii) since tensors are a natural representation of weights and gradients in most 
deep learning code bases. Hence, e.g., left an right matrix-multiplication can be thought of as
a Kronecker multiplication as in Eq. \eqref{eq:vec}.
Two recent, but not the first, examples are K-FAC <a href="#references">[4]</a> and Shampoo <a href="#references">[5]</a>, but there are
many more, also older ones that explore Kronecker structure in the context of stochastic optimization.</p>

<h2 id="references">References</h2>

<p>[1] C.F. Van Loan 2000 <em>The ubiquitous Kronecker product</em>, 
    Journal of Computational and Applied Mathematics 123, pp. 85–100.</p>

<p>[2] C.F. Van Loan and N. Pitsianis 1993 <em>Approximation with Kronecker Products</em>,
    Linear Algebra for Large Scale and Real Time Applications. Kluwer Publications, pp. 293–314.</p>

<p>[3] M. Mahsereci 2018 <em>Probabilistic Approaches to Stochastic Optimization</em>, PhD thesis, Appendix A.</p>

<p>[4] J. Martens and r. Grosse 2015 <em>Optimizing Neural Networks with Kronecker-factored Approximate Curvature</em>, ArXiv.</p>

<p>[5] V. Gupta et al. 2018 <em>Shampoo: Preconditioned Stochastic Tensor Optimization</em>, ICML.</p>

<p>[6] P. Hennig 2015 <em>Probabilistic Interpretation of Linear Solvers</em>, SIAM.</p>


  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    © Copyright 2024 Maren  Mahsereci.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
