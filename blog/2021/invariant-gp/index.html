<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Maren  Mahsereci


  | Invariant Gaussian Processes

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>Ω</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2021/invariant-gp/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://mmahsereci.github.io/">
       <span class="font-weight-bold">Maren</span>   Mahsereci
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              <!-- I changed this -->
              posts
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      





<div class="post">

  <header class="post-header">
    <h1 class="post-title">Invariant Gaussian Processes</h1>
    <p class="post-meta">November 1, 2021 • mmahsereci</p>
    <p class="post-tags">
      <a href="/blog/2021"> <i class="fas fa-calendar fa-sm"></i> 2021 </a>
      
        ·  
        
        <a href="/blog/tag/gaussianprocesses">
          <i class="fas fa-hashtag fa-sm"></i> gaussianprocesses</a>  
          
        <a href="/blog/tag/machinelearning">
          <i class="fas fa-hashtag fa-sm"></i> machinelearning</a>  
          
      

      
        ·  
        
        <a href="/blog/category/techblog">
          <i class="fas fa-tag fa-sm"></i> techblog</a>  
          
      

    </p>
  </header>

  <article class="post-content">
    <p>Gaussian processes can be understood as “distributions over functions” providing prior models for
unknown functions. The kernel which identifies the GP can be used to encode known properties of the function such 
as smoothness or stationarity. A somewhat more exotic characteristic is <em>invariance to input transformations</em> 
which we’ll explore here.</p>

<h2 id="what-is-an-invariant-function">What is an invariant function?</h2>

<p>I came across the paper of <a href="https://proceedings.neurips.cc/paper/2018/file/d465f14a648b3d0a1faa6f447e526c60-Paper.pdf" target="_blank" rel="noopener noreferrer">Wilk et al. 2018</a> 
a while ago which introduces Gaussian processes that are invariant
under a finite set of input transformations. 
Let’s first see what an invariant function is: A function \(f:\mathcal{X} \rightarrow \mathbb{R}\) on the domain \(\mathcal{X}\subseteq\mathbb{R}^D\)
is said to be invariant under a bijective transformation 
\(T:\mathcal{X}\rightarrow \mathcal{X}\) if 
\(f(T(x)) = f(x)\) holds for all \(x\) in \(\mathcal{X}\).
This simply means that the function \(f\) takes the same value at \(x\) and \(T(x)\) for all \(x\).</p>

<blockquote>
  <p>Simple 1D example: \(f(x) = x^2\) is invariant under flipping the x-axis, i.e., \(f(x) = f(-x)\) with \(T(x) = -x\), 
and \(x\in\mathbb{R}\).</p>
</blockquote>

<h3 id="invariance-groups-and-orbits">Invariance groups and orbits</h3>

<p>Consider now a function that is invariant under a finite set of \(J\) transformations \(T_i\), \(i=1, ..., J\).
As the invariance under each \(T_i\) holds for any input \(x\) (also those that have been transformed), 
the \(T_i\) must form the group 
\(G_f:=\{T | f(x) = f(T(x)) \text{ for all } x \in\mathcal{X}\}\). 
That is, \(G_f\) contains arbitrary concatenations \(T_i\circ T_j\circ \dots\), the identity transform \(T=I\),
the inverses \(T_i^{-1}\), and the \(T_i\) obey associativity. 
There may be several groups associated with a function \(f\), depending on which invariances are considered.</p>

<blockquote>
  <p>For the above example \(f(x) = x^2\), the implied group \(G_f\) only contains \(J=2\)
transformations \(G_f = \{T_0, T_1\}\) with \(T_0(x)=x\) and \(T_1(x)=-x\). 
This is because all possible concatenations  \(T_i\circ T_j\circ \dots\) as well as the inverses \(T_i^{-1}\) 
collapse back to \(T_0\) or \(T_1\) and are thus already in \(G_f\) (Examples: \(T_1^{-1} = T_1\), or \(T_1\circ (T_1 \circ T_1) = T_1\circ T_0 = T_1\) etc).</p>
</blockquote>

<p>Given \(G_f\), the set \(\mathcal{A}(x):=\{T(x) | \text{ for all } T\in G_f\}\) 
is called an <em>orbit</em> of \(x\) and is the set of invariant locations induced by \(x\).</p>

<blockquote>
  <p>In our 1D example \(f(x) = x^2\) with \(G_f\) containing \(T_0\) and \(T_1\), consider an arbitrary point 
e.g., \(x^* = 0.2\). The orbit of \(x^*\) is then given by \(\mathcal{A}(x^*) = \{T_0(x^*), T_1(x^*)\} = \{0.2, -0.2\}\).</p>
</blockquote>

<h2 id="modelling-an-invariant-black-box-function">Modelling an invariant black-box function</h2>

<p>Given \(G_f\) and \(\mathcal{A}(x)\), we can now introduce a latent function \(g: \mathcal{X} \rightarrow \mathbb{R}\) such that</p>

\[f(x) = \sum_{\tilde{x}\in \mathcal{A}(x)}g(\tilde{x}).\]

<p>The latent function \(g\) is not necessarily invariant, but the function \(f\) is by construction.
This is because any point \(\tilde{x}\in \mathcal{A}(x)\) induces the identical set 
\(\mathcal{A}(\tilde{x}) = \mathcal{A}(x)\), hence \(f(\tilde{x}) = f(x)\) 
for all \(\tilde{x}\in A(x)\).
(notice that \(G_f\) includes the identity transform, the trivial invariance for all functions).</p>

<p>We see from the above equation that we do not need to know the form of \(f(x)\) in order to encode the 
invariance property, simple knowing the set \(G_f\) is enough. In that sense, \(f(x)\) can be a black-box function.</p>

<h3 id="making-it-probabilistic-an-invariant-gaussian-process-prior">Making it probabilistic (An invariant Gaussian process prior)</h3>

<p>If the function \(f\) is unknown, and we want to learn it from a set of function evaluations, we can treat the 
inference problem probabilistically to account for the uncertainty of value of \(f\) where \(f(x)\) is not observed.
In certain circumstance, a good choice of models are Gaussian processes (GPs), especially since they let us 
encode properties of the function leading to sample efficient learning methods.
In our case, the property we want to encode is the invariance of \(f\) under the group \(G_f\) as defined above. 
We again follow <a href="https://proceedings.neurips.cc/paper/2018/file/d465f14a648b3d0a1faa6f447e526c60-Paper.pdf" target="_blank" rel="noopener noreferrer">Wilk et al. 2018</a> (Section 4.1).</p>

<p>In essence, the trick is to model the latent function \(g\) as a Gaussian process \(g\sim\mathcal{G}(m_g, k_g)\) with mean function \(m_g\) and 
kernel function \(k_g\) such that the resulting process on \(f\) obeys the invariance by the above construction.
It turns out that then, \(f\) is also a Gaussian process \(f\sim\mathcal{G}(m_f, k_f)\), as \(f\) is a linear combination 
(weighted sum) of jointly Gaussian distributed values \(g\) (and Gaussians are closed under linear transformations). 
The mean function \(m_f\) and kernel function \(k_f\) can be easily derived:</p>

\[m_f(x) =\sum_{\tilde{x}\in \mathcal{A}(x)}m_g(\tilde{x}),\qquad
    k_f(x, x') = \sum_{\tilde{x}', \tilde{x}\in \mathcal{A}(x)}  k_g(\tilde{x}, \tilde{x}').\]

<p>GP regression on \(f\) is straightforward, too, as the above equation simply defines another positive definite kernel \(k_f\).
In essence, we did not transform the distribution of \(g\) itself, we merely correlated function values of \(g\) at invariant locations in input space.</p>

<h2 id="pretty-priors-samples-of-invariant-gps">Pretty Priors: Samples of invariant GPs</h2>

<p>Let’s create some plots. Below we plot 4 x 8 = 32 samples of invariant Gaussian processes with a 2D input domain.
The samples are from the prior GP not conditions on any data. We can see that the samples obey the invariances encoded.
This means that the model, if conditioned on function evaluation, needs not learn the invariance property 
of the function from the data, as it is already encoded in the prior. This will likely lead to sample-efficient algorithms.</p>

<h3 id="point-symmetry">Point-symmetry</h3>

<p>Group \(G_f\) is of size \(J=2\) and contains \(T_0= I\) as well as a projection operator though the origin 
(flipping all signs) \(T_1 = -1\). This encodes point-symmetry of \(f\) through the origin as can be seen from the 
prior samples (the origin at \([0, 0]\) is in the center of each plot).</p>

<div style="text-align:center">
  <img src="/assets/posts/2021-11-01-invariant-gp/point_00_a.png" style="width:90%; padding-top: 10px; padding-bottom: 10px;">
</div>

<h3 id="axis-symmetry-along-both-axis">Axis-symmetry along both axis</h3>

<p>In order to encode axis-symmetry along both axis, we need to add two additional invariances \(T_2\) and \(T_3\) in addition to \(T_0\) and
\(T_1\) above (total of \(J=4\) transformations). \(T_2\) and \(T_3\) each flip the sign of one axis, 
that is \(T_3 = [[-1, 0]; [0, 1]]\) and \(T_3 = [[1, 0]; [0, -1]]\).
We observe that the samples from this prior observe the axis-symmetries.</p>

<div style="text-align:center">
  <img src="/assets/posts/2021-11-01-invariant-gp/flip-point_00_a.png" style="width:90%; padding-top: 10px; padding-bottom: 10px;">
</div>

<h3 id="axis-symmetry-along-one-axis">Axis-symmetry along one axis</h3>

<p>For axis-symmetry along one axis only (say x-axis), we only require \(J=2\) transformations given by \(T_0=I\) and 
\(T_1 = [[1, 0]; [0, -1]]\). The samples again obey the symmetry.</p>

<div style="text-align:center">
  <img src="/assets/posts/2021-11-01-invariant-gp/x-flip_00_a.png" style="width:90%; padding-top: 10px; padding-bottom: 10px;">
</div>

<h3 id="rotations">Rotations</h3>

<p>Lastly, for \(J\)-fold rotational symmetry, the group \(G_f\) contains \(J\)-fold rotation matrices with 
\(T_i = R(\theta=i\frac{2\pi}{J})\) for \(i=1,\dots,J\) where \(R(\theta)\) is 
the <a href="https://en.wikipedia.org/wiki/Rotation_matrix" target="_blank" rel="noopener noreferrer">2D rotation matrix</a>. Below we show \(J=5\).</p>

<div style="text-align:center">
  <img src="/assets/posts/2021-11-01-invariant-gp/rotations_00_a.png" style="width:90%; padding-top: 10px; padding-bottom: 10px;">
</div>

<p>Same as above but for \(10\)-fold rotation (\(J=10\)).</p>

<div style="text-align:center">
  <img src="/assets/posts/2021-11-01-invariant-gp/rotations_00_n10_a.png" style="width:90%; padding-top: 10px; padding-bottom: 10px;">
</div>

<h2 id="where-is-the-data">Where is the data?</h2>

<p>Nowhere yet. I may write another blogpost showing posterior samples from the invariant GPs. They look quite cool 
indeed, as the invariant GP seems to “learn” at locations where nothing is observed (this is at and close to points 
that are invariant to observed points under the model). I may also discuss algorithmic complexity there.</p>

<h2 id="references">References</h2>

<p>[1] Wilk et al. 2018 <em>Learning invariances using the marginal likelihood</em>, 
    Advances in Neural Information Processing Systems 31, pages 9938–9948.</p>

<p>[2] Rasmussen and Williams 2006. <em>Gaussian Processes for Machine Learning.</em> Adaptive Computation and Machine Learning. 
    MIT Press, Cambridge, MA, USA.</p>

  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    © Copyright 2024 Maren  Mahsereci.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
