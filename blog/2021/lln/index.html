<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Maren  Mahsereci


  | The Law of Large Numbers

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>Ω</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2021/lln/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://mmahsereci.github.io/">
       <span class="font-weight-bold">Maren</span>   Mahsereci
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              <!-- I changed this -->
              posts
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      





<div class="post">

  <header class="post-header">
    <h1 class="post-title">The Law of Large Numbers</h1>
    <p class="post-meta">August 5, 2021 • mmahsereci</p>
    <p class="post-tags">
      <a href="/blog/2021"> <i class="fas fa-calendar fa-sm"></i> 2021 </a>
      
        ·  
        
        <a href="/blog/tag/statistics">
          <i class="fas fa-hashtag fa-sm"></i> statistics</a>  
          
      

      
        ·  
        
        <a href="/blog/category/techblog">
          <i class="fas fa-tag fa-sm"></i> techblog</a>  
          
      

    </p>
  </header>

  <article class="post-content">
    <p>The law of large numbers (LLM) is one of the most fundamental theorems of statistics. 
It states that the average value of a repeatedly performed random experiment tends to the mean value. 
But what does this mean? And why is it so powerful?</p>

<p>Let \(x\) be a random variable with distribution \(p(x)\), and let \(x_1, \dots, x_n\) be \(n\) random draws from \(p(x)\).
Denote the mean of \(x\) by \(\mu : = \mathbb{E}[x]\) and define the mean statistic for a sample of size \(n\) as 
\(\bar{x}_n : = \frac{1}{n}\sum_{i=1}^n x_i\).</p>

<p>Then, the law of large numbers states that</p>

\[\bar{x}_n\xrightarrow[n \to \infty]{} \mu\]

<p>that is for a large enough sample size \(n\), the mean statistic tends towards the population mean.</p>

<p>This is a remarkable statement since, simply by random sampling, one can estimate the average value of a whole 
polulation. For instance, it might be infeasible to compute \(\mu\) directly if the population is very large
but we can compute the average of a much smaller random sample instead.</p>

<p>The question arises now, how large \(n\) needs to be to get a somewhat reliable estimate? For now we’ll 
give an empirical answer for the special case of coin tosses. The concept of coin tosses is more general than 
the name suggests, as the same technique is applied when computing counts or percentages e.g., when asking 
which percentage \(\mu\) of all voters support the current government. In those cases “heads” would 
mean that a random voter supports the government, and “tails” would mean that they do not. But for the sake of this
blog post we’ll stick with the analogy of coin tosses.</p>

<h3 id="tossing-a-coin-n-times">Tossing a coin \(n\) times</h3>

<p>We’ll consider the case of a fair coin \(\mu=0.5\) and produce Bernoulli random samples \(x_1, \dots, x_n \in \{0, 1\}\). 
This means, “heads” will come up with probability \(\mu\) and “tails” with probability \(1-\mu\).
We’ll assign the value “1” to  “heads”, and “0” to “tails”.</p>

<p>In practice, we would only have access to the sample
\(x_1, \dots, x_n\), and the statistic \(\bar{x}_n\) would be used to estimate the unknown parameter \(\mu\). 
But in this blog post we want to illustrate how fast \(\bar{x}_n\) gets close to \(\mu\), hence we simulated the sample
from the distribution with parameter \(\mu\) and then check how well our estimator does.
We first draw the sample:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># fair coin
</span><span class="n">mu</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="c1"># Bernoulli samples: 1 means heads, 0 means tails
</span><span class="n">n_samples</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="mf">1e5</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">mu</span><span class="p">)</span>

<span class="c1"># print the first 10 samples
</span><span class="nf">print</span><span class="p">(</span><span class="n">samples</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([1, 0, 0, 0, 1, 1, 1, 0, 0, 0])
</code></pre></div></div>

<p>We now compute the mean statistic \(\bar{x}_n\) for an increasing sample size \(n=1,\dots,1e^5\).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span>  <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">means</span> <span class="o">=</span> <span class="n">S</span> <span class="o">/</span> <span class="n">n</span>
</code></pre></div></div>

<p>We plot \(\bar{x}_n\) over \(n\) in blue, and the true parameter \(\mu\) in gray as horizontal line. 
The x-axis is in log-scale.
We see that \(\bar{x}_n\) gets quite close to \(\mu\) for \(n\) larger than roughly 100. 
This is an astonishingly small number considering that we could estimate real parameters such as voter percentages with 
the same technique. 
Of course the real world is a bit more tricky, but the general observation holds that sample sizes \(n\) often need not 
be overly large such that statistics \(\bar{x}_n\) somewhat accurately represent parameters of quite large populations.</p>

<div style="text-align:center">
  <img src="/assets/posts/2021-08-05-lln/00.png" style="width:90%; padding-top: 10px; padding-bottom: 10px;">
</div>

<h2 id="the-strong-and-weak-lln">The strong and weak LLN</h2>

<p>We did not specify above what the statement \(\bar{x}_n\xrightarrow[n \to \infty]{} \mu\) means precisely. 
It’s also not super straightforward to define since the sequence \(\bar{x}_n\) is not deterministic 
(remember, we’re tossing coins and averaging the outcome).
In fact, there are typically two definitions.</p>

<p>The first one is the weak law of large numbers (weak LLN) which implies convergence in probability, that is
\(\lim_{n\rightarrow \infty}P(|\bar{x}_n - \mu| &gt; \epsilon)=0\) for all \(\epsilon &gt; 0\). In words this means that 
for any \(\epsilon, \delta &gt; 0\) there exists an \(n\) (that depends on \(\delta\) and \(\epsilon\)) such that 
\(\bar{x}_n\) will be inside a ball of radius \(\epsilon\) centered at \(\mu\) with probability \(1-\delta\).
That means that any random sequence will eventually have a high probability to be close to \(\mu\); although for 
finite \(n\) we can never quite box <em>all</em> \(\bar{x}_n\) inside the ball, only with high probability \(1-\delta\).</p>

<p>The second one is the strong law of large numbers (strong LLN) 
which implies that \(\bar{x}_n\) converges to \(\mu\) almost surely that is 
\(P(\lim_{n\rightarrow \infty} \bar{x}_n = \mu) = 1\). 
It means that the probability of the event for which the sequence \(\bar{x}_n\) does not converge to \(\mu\) in the
classic sense is zero.
This is a stronger statement as it implies that there exists an \(n\) for which the probability for <em>all</em> 
\(\bar{x}_{m&gt;n}\) to be outside a ball of radius \(\epsilon\) is zero, i.e, we can box in all \(\bar{x}_n\) with 
probability 1 at some point.
Any strongly convergent sequence is also weakly convergent.</p>

<p>In practice, both of the above statements mean that for large enough \(n\), the sample mean \(\bar{x}_n\) can represent 
the population mean \(\mu\) quite well as we have seen in the plot.
In terms of tossing a fair coin, and somewhat pictorially, convergence means that the event of any sequence 
of “heads” and “tails” that does not average to half “heads” and half “tails” in the long run, such as e.g., 
only “heads” \([1, 1, 1, 1, \dots]\) has vanishing probability.</p>

<h2 id="what-is-the-catch">What is the catch?</h2>
<p>There is always a catch of course. And in the case of LLN it is probably that it requires us to draw truly random
numbers from \(p(x)\). In the example above it was easy to do as Numpy provides us with a pseudo-random number 
generator. 
However, if \(p(x)\) relates to the real world, and is less accessible, producing a random sample is 
often not easy at all. In this case the sample can have several unknown biases, e.g., selection bias, and the statement 
\(\bar{x}_n\xrightarrow[n \to \infty]{} \mu\) does not hold anymore. 
But this is a topic for another blog post :)</p>


  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    © Copyright 2024 Maren  Mahsereci.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
