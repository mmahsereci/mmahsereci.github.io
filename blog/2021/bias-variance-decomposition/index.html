<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Maren  Mahsereci


  | The Bias-Variance Decomposition

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>Ω</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2021/bias-variance-decomposition/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://mmahsereci.github.io/">
       <span class="font-weight-bold">Maren</span>   Mahsereci
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              <!-- I changed this -->
              posts
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      





<div class="post">

  <header class="post-header">
    <h1 class="post-title">The Bias-Variance Decomposition</h1>
    <p class="post-meta">October 31, 2021 • mmahsereci</p>
    <p class="post-tags">
      <a href="/blog/2021"> <i class="fas fa-calendar fa-sm"></i> 2021 </a>
      
        ·  
        
        <a href="/blog/tag/statistics">
          <i class="fas fa-hashtag fa-sm"></i> statistics</a>  
          
        <a href="/blog/tag/machinelearning">
          <i class="fas fa-hashtag fa-sm"></i> machinelearning</a>  
          
      

      
        ·  
        
        <a href="/blog/category/techblog">
          <i class="fas fa-tag fa-sm"></i> techblog</a>  
          
      

    </p>
  </header>

  <article class="post-content">
    <p>The expected squared error of an estimator can be decomposed into a bias and a variance term.
In order for the error to be minimal, generally not both, bias and variance, can be minimal.
This is the bias-variance trade-off.</p>

<h3 id="the-data">The data</h3>
<p>Consider a dataset</p>

\[\mathcal{D} = \{(x_1,y_1),\dots,(x_n, y_n)\}\]

<p>with inputs \(x\) and targets \(y\). 
Assume that the targets are generated by the relation</p>

\[\begin{equation}
\label{eq:y}
y = f(x) + \epsilon
\end{equation}\]

<p>where
\(f(x)\) is a deterministic, but unknown map.</p>

<p>The scalar \(\epsilon\sim p(\epsilon)\) is an additive noise term
with zero mean \(\operatorname{Exp}[\epsilon]=0\) and finite variance 
\(\operatorname{Var}[\epsilon]=\sigma^2\). 
More generally, we can assume some data-generating process \((x, y)\sim p(x, y)\)
that is obeying Eq. \eqref{eq:y}, and by extension a distribution over datasets
of size \(N\)</p>

\[\begin{equation}
\label{eq:D}
\mathcal{D} \sim p(\mathcal{D}),
\end{equation}\]

<p>one realization of which is the dataset at hand 
(\(\mathcal{D}\) hence denotes both, the given dataset and the random variable).</p>

<h3 id="the-estimator">The estimator</h3>

<p>Since the relation \(f\) in Eq. \eqref{eq:y} is unknown, we would like to estimate it from the 
given dataset \(\mathcal{D}\).</p>

<p>Hence, consider a model \(\hat{f}_{\mathcal{D}}\) that is fitted to \(\mathcal{D}\)
and can produce a prediction \(\hat{f}_{\mathcal{D}}(x)\) for some given \(x\),
where \(x\) is not necessarily in the dataset.
This prediction estimates (guesses, based on data and model assumptions) the unknown value \(f(x)\). 
The function \(f_{\mathcal{D}}(x)\) is called an <em>estimator</em> for \(f(x)\).</p>

<p>Further, if we take Eq. \eqref{eq:D} seriously and assume that \(\mathcal{D}\) is a 
random variable, by extension, the estimator \(\hat{f}_{\mathcal{D}}(x)\) 
is a random variable, too.</p>

<h2 id="what-is-the-bias-of-an-estimator">What is the bias of an estimator?</h2>

<p>The bias of the estimator \(\hat{f}_{\mathcal{D}}(x)\) is defined as the difference between its 
expectation w.r.t. \(p(\mathcal{D})\) and the ground truth value \(f(x)\)</p>

\[\operatorname{Bias}_{p(\mathcal{D})}{[\hat{f}_{\mathcal{D}}(x)]} 
= \operatorname{Exp}_{p(\mathcal{D})}[\hat{f}_{\mathcal{D}}(x)] - f(x).\]

<p>The bias thus tells us how much the prediction \(\hat{f}_{\mathcal{D}}(x)\) deviates from the true 
value \(f(x)\) on average, where the average is taken over all potential datasets
\(\mathcal{D}\) weighted with the probability they occur.
If the bias is zero, it means that the model on average yields the correct prediction.
Keep in mind tough, that the bias is a theoretical construct involving the ground truth \(f(x)\)
and is generally not tractable.</p>

<p>While the bias tells us about the goodness of the average prediction, 
it does not tell us how predictions \(\hat{f}_{\mathcal{D}}(x)\) for individual datasets
may differ from one another. This is encoded in the variance of \(\hat{f}_{\mathcal{D}}(x)\) which we discuss next.</p>

<h2 id="what-is-the-variance-of-an-estimator">What is the variance of an estimator?</h2>

<p>Again assume all the above, and especially \(\mathcal{D}\sim p(\mathcal{D})\).
The variance of the estimator \(\hat{f}_{\mathcal{D}}(x)\) is defined as</p>

\[\operatorname{Var}_{p(\mathcal{D})}{[\hat{f}_{\mathcal{D}}(x)]} 
= \operatorname{Exp}_{p(\mathcal{D})}\left[\left(\hat{f}_{\mathcal{D}}(x) 
- \operatorname{Exp}_{p(\mathcal{D})}[\hat{f}_{\mathcal{D}}(x)]\right)^2\right].\]

<p>Hence, the variance is the expected square deviation of the estimator from its mean.
While the bias of \(\hat{f}_{\mathcal{D}}(x)\) depends on the ground truth \(f(x)\), the variance does not;
it simply quantifies the spread of the individual estimators.
However, like the bias, the variance is usually not tractable either since the
expectation over \(p(\mathcal{D})\) cannot be computed.</p>

<h2 id="the-bias-variance-decomposition-of-the-expected-squared-error">The bias-variance decomposition of the expected squared error</h2>

<p>The bias and variance each describe one aspect of the estimator \(\hat{f}_{\mathcal{D}}(x)\) 
and each individually give an incomplete picture of its behavior. 
But, do they give a complete picture together? The answer is generally no, but there is 
one interesting exception:</p>

<blockquote>
  <p>Assuming the data-generation as above, the expected squared error of the estimator \(\hat{f}_{\mathcal{D}}(x)\)
can be decomposed into its bias squared, its variance and an independent, irreducible error that depends on the observation noise \(\epsilon\).</p>
</blockquote>

<p>Consider the squared error (square loss)</p>

\[\ell(x, y; \hat{f}_{\mathcal{D}}) = \left(\hat{f}_{\mathcal{D}}(x) - y\right)^2\]

<p>as a measure for the performance of \(\hat{f}_{\mathcal{D}}(x)\) on the pair \((x, y)\).
The highlighted statement above means that the expected squared error can be decomposed 
as follows:</p>

\[\begin{equation}
\label{eq:decomp}
\begin{split}
\operatorname{Exp}_{p(\mathcal{D})}[\ell(x, y; \hat{f}_{\mathcal{D}})]
&amp;= \operatorname{Exp}_{p(\mathcal{D})}\left[\left(\hat{f}_{\mathcal{D}}(x) - y\right)^2\right]\\
&amp;= \left(\operatorname{Bias}_{p(\mathcal{D})}{[\hat{f}_{\mathcal{D}}(x)(x)]}\right)^2
+ \operatorname{Var}_{p(\mathcal{D})}{[\hat{f}_{\mathcal{D}}(x)(x)]} 
+ \sigma^2,
\end{split}
\end{equation}\]

<p>where \(\sigma^2=\operatorname{Var}[\epsilon]\) is the irreducible error.
This is the bias-variance decomposition of the expected squared error.</p>

<p>The derivation is straightforward and can be done by expanding terms and applying the 
expectation where possible.</p>

<h3 id="the-bias-variance-trade-off">The bias-variance trade-off</h3>

<p>The expected squared error, as well as all summands in the second line of Eq. \eqref{eq:decomp} are non-negative.
The usual intuition is that, when varying the estimator \(\hat{f}_{\mathcal{D}}(x)\), if its squared bias
gets smaller, its variance gets larger and vice versa.
This means that there is an optimal 
\(\hat{f}^*_{\mathcal{D}}(x)\) where the expected squared error, but generally neither the squared bias, nor variance is minimal.
This is the bias-variance trade-off. In formulas, if</p>

\[\begin{equation*}
\begin{split}
\hat{f}^*_{\mathcal{D}}(x)
&amp; = \operatorname*{arg\,min}_{\hat{f}_{\mathcal{D}}(x)}\operatorname{Exp}_{p(\mathcal{D})}\left[\left(\hat{f}_{\mathcal{D}}(x) - y\right)^2\right]\\
\end{split}
\end{equation*}\]

<p>then in general</p>

\[\begin{equation*}
\begin{split}
\hat{f}^*_{\mathcal{D}}(x)
&amp;\neq \operatorname*{arg\,min}_{\hat{f}_{\mathcal{D}}(x)}\left(\operatorname{Bias}_{p(\mathcal{D})}{[\hat{f}_{\mathcal{D}}(x)(x)]}\right)^2\\
\hat{f}^*_{\mathcal{D}}(x)
&amp;\neq \operatorname*{arg\,min}_{\hat{f}_{\mathcal{D}}(x)}\operatorname{Var}_{p(\mathcal{D})}{[\hat{f}_{\mathcal{D}}(x)(x)]}.
\end{split}
\end{equation*}\]

<p>Varying the estimator here may mean to use a different model for \(\hat{f}_{\mathcal{D}}\)
or a different fitting procedure, or more generally to vary the deterministic mechanism that
retrieves \(\hat{f}_{\mathcal{D}}\) from \(\mathcal{D}\).
The irreducible error \(\sigma^2\) does not play a role as it is a constant
w.r.t. \(\hat{f}_{\mathcal{D}}\).</p>

<p>The above (in)equalities suggest that, at least under the square loss (and possibly elsewhere, too), a non-zero bias 
of an estimator is not a bad thing if one is interested in its predictive performance.</p>

<h2 id="illustrative-example">Illustrative example</h2>

<p>The bias-variance trade-off is often explained with the
expressiveness of the model \(\hat{f}_{\mathcal{D}}\) and its flexibility to fit the data. 
For instance, a simple model such as linear regression with a small amount of features
may yield similar predictions no matter the dataset \(\mathcal{D}\) in which case its variance is
low, but its bias is large as it never can fully express the ground truth \(f(x)\). 
An expressive model on the other hand—let’s say linear regression with many features—may 
be able to represent \(f(x)\) and even “patterns” in \(\mathcal{D}\) that are due to the noise term \(\epsilon\). 
In this case, the bias may be low, but the variance may be high.
The latter is often associated with “overfitting” (model is too flexible with the data) and the 
former with “underfitting” (model is too rigid), although underfitting is a less well-defined term.</p>

<p>As an illustrative example, consider the function</p>

\[\begin{equation}
\label{eq:g}
g(x, K) = w^{\intercal} \Phi(x) = \sum_{k=1}^{2K+1}w_k\Phi_k(x)
\end{equation}\]

<p>where \(\Phi(x) = [1, \sin(x), \cos(x), \sin(2x), \cos(2x), \dots, \sin(Kx), \cos(Kx)]\in\mathbb{R}^{2K+1}\) are
some Fourier features and \(w=[w_1, w_2, \dots, w_{2K+1}]\in\mathbb{R}^{2K+1}\)
are some fixed parameters.</p>

<h3 id="the-ground-truth-function">The ground truth function</h3>

<p>As ground truth function, we use \(K=3\), that is \(f(x):=g(x, K=3)\) and some fixed parameters \(w\).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">_features_fourier</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">K</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Computes K Fourier features for x.</span><span class="sh">"""</span>
    <span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">([</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nf">int</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">K</span><span class="p">)])</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">xi</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">features_i</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">]</span>        
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">features_i</span> <span class="o">=</span> <span class="n">features_i</span> <span class="o">+</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">k</span> <span class="o">*</span> <span class="n">xi</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">k</span><span class="o">*</span><span class="n">xi</span><span class="p">)]</span>
        <span class="n">features</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">features_i</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">features</span>


<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">The ground truth function.</span><span class="sh">"""</span>
    <span class="c1"># Fourier features for K=3
</span>    <span class="n">ff</span> <span class="o">=</span> <span class="nf">_features_fourier</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="c1"># 2*K + 1 fixed parameters w
</span>    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span> <span class="mf">0.49671415</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1382643</span> <span class="p">,</span>  <span class="mf">0.64768854</span><span class="p">,</span>  <span class="mf">1.52302986</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.23415337</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.23413696</span><span class="p">,</span>  <span class="mf">1.57921282</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">ff</span> <span class="o">@</span> <span class="n">w</span>
</code></pre></div></div>

<p>The resulting ground truth function \(f(x)\) is plotted below on the x-range \([0, 10]\).</p>

<div style="text-align:center">
  <img src="/assets/posts/2021-10-31-bias-variance-decomposition/00.png" style="width:90%; padding-top: 10px; padding-bottom: 10px;">
</div>

<h3 id="the-data-1">The data</h3>

<p>We know the data must obey Eq. \eqref{eq:y} with an \(\epsilon\) that has zero mean 
and finite variance \(\operatorname{Var}[\epsilon]=\sigma^2\). 
Hence, we choose to produce each dataset according to:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample_dataset</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
    <span class="n">F</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">F</span> <span class="o">+</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">F</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">F</span> <span class="o">=</span> <span class="nf">sample_dataset</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
</code></pre></div></div>

<p>Each dataset is of size \(N=50\) in this example. 
Since we want to illustrate the intractable bias of an estimator we also return the unperturbed 
ground truth values \(f(x)\) (stored in the array F) which we would not know in practice. 
Similarly, \(\sigma\) may or may not be known in practice.
One example of a dataset is plotted below (blue dots scattered) with the ground truth function as dotted line.
The datapoints do not exactly lie on top of the function due to the noise term \(\epsilon\).
A good estimator should avoid fitting those “wiggles” and rather
smooth them out to fit the dotted line more closely.</p>

<div style="text-align:center">
  <img src="/assets/posts/2021-10-31-bias-variance-decomposition/00data.png" style="width:90%; padding-top: 10px; padding-bottom: 10px;">
</div>

<p>Further, we denote the set of nodes, observations and ground truth values as 
\(X=[x_1,\dots,x_N]\in\mathbb{R}^{N}\),
\(Y=[y_1,\dots,y_N]\in\mathbb{R}^{N}\) and
\(F=[f(x_1),\dots,f(x_N)]\in\mathbb{R}^{N}\) respectively.
The feature matrix is denoted as \(\Phi(X)\in\mathbb{R}^{N\times (2K + 1)}\)
where \([\Phi(X)]_{ik} = \Phi_k(x_i)\) with \(i=1,\dots,N\) and \(k=1,\dots, (2K+1)\).</p>

<h3 id="estimators-with-increasing-complexity">Estimators with increasing complexity</h3>

<p>For our illustrative purposes we create 14 estimators with increasing complexity 
that have the functional form as in Eq. \eqref{eq:g} with \(K\) ranging from 0 to 13. 
The estimators are then fitted to the data via least squares. That is, the  \(w\) parameters 
are the minimizers of the empirical square loss</p>

\[\hat{w} 
= \operatorname*{arg\,min}_{w} \sum_{(x, y)\in\mathcal{D}}\left(y - w^{\intercal}\Phi(x)\right)^2\]

<p>which admits an analytic solution \(\hat{w} = [\Phi(X)^{\intercal}\Phi(X)]^{-1}\Phi(X)^{\intercal}Y\).
To make the dependence on \(K\) explicit, we write</p>

\[\begin{equation}
\label{eq:fhat}
\begin{split}
\hat{w}(K) &amp;= [\Phi(X, K)^{\intercal}\Phi(X, K)]^{-1}\Phi(X, K)^{\intercal}Y \in \mathbb{R}^{2K+1}\\
\hat{f}_{\mathcal{D}}(x, K) &amp;= \hat{w}(K)^{\intercal}\Phi(x, K).
\end{split}
\end{equation}\]

<p>This means, our 13 estimators are the functions 
\(\hat{f}_{\mathcal{D}}(x, K=0), \hat{f}_{\mathcal{D}}(x, K=1),\dots,\hat{f}_{\mathcal{D}}(x, K=13)\)
with increasing complexity.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_solve_linear_least_squares</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">lstsq</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_weights</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Phi</span><span class="p">):</span>
    <span class="n">features</span> <span class="o">=</span> <span class="nc">Phi</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">gram</span> <span class="o">=</span> <span class="n">features</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">features</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">features</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Y</span>
    <span class="k">return</span> <span class="nf">_solve_linear_least_squares</span><span class="p">(</span><span class="n">gram</span><span class="p">,</span> <span class="n">b</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">fhat_from_weights</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">Phi</span><span class="p">):</span>
    <span class="n">features</span> <span class="o">=</span> <span class="nc">Phi</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">features</span> <span class="o">@</span> <span class="n">w</span>    

<span class="c1"># Example of fitting the estimator with K = 5 to the data X, Y
</span><span class="n">K</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">Phi</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">_features_fourier</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
<span class="n">w_hat</span> <span class="o">=</span> <span class="nf">compute_weights</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Phi</span><span class="p">)</span>
<span class="n">fhat</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">fhat_from_weights</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w_hat</span><span class="p">,</span> <span class="n">Phi</span><span class="p">)</span>

<span class="c1"># evaluate on random test point
</span><span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">atleast_1d</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">f_hat_at_x_test</span> <span class="o">=</span> <span class="nf">fhat</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="estimation-of-bias-and-variance">Estimation of bias and variance</h3>

<p>Recall that the bias and the variance of an estimator are generally not tractable.
For each of the 14 estimators, we thus approximate those two quantities by computing the expectations
over \(p(\mathcal{D})\) with Monte Carlo using \(10^3\) samples. This means, for each estimator, 
we draw \(10^3\) datasets of size \(N=50\), we fit the estimator to each dataset with
Eq. \eqref{eq:fhat}. This yields \(10^3\) predictions on a test point which we
choose randomly to be \(x\approx 8.96\). The results are then used to approximate the expectations 
required to compute the bias and the variance term.</p>

<p>The plot below shows the squared bias, the variance, the expected loss and \(\sigma^2\) versus
\(K\) of the estimators which is a proxy for the model complexity.
The “ground truth model”, meaning the estimator which uses the same \(K\) as the ground
truth function \(f\) is marked with a vertical dotted line at \(K=3\).
Unsurprisingly, it also happens to be the best estimator as it exhibits the minimal expected error 
on the test point. As expected, neither its variance, nor its squared bias is minimal
when compared to all other estimators.</p>

<div style="text-align:center">
  <img src="/assets/posts/2021-10-31-bias-variance-decomposition/05.png" style="width:90%; padding-top: 10px; padding-bottom: 10px;">
</div>

<p>The behavior of the estimators can be understood by looking at individual fits.
The following plots shows fits of all 14 estimators to one of the datasets of size \(N=50\).
The estimator with minimal expected squared error (with \(K=3\)) is shown in orange.
It is apparent that less complex estimators with a lower \(K\) are too rigid to fit
the underlying function well which explains the large bias and low variance term, while
the more complex models even fit to the noise term \(\epsilon\) which explains their large variance. 
The winning model with \(K=3\) has just the right complexity to explain the signal, but 
smooth out the perturbations \(\epsilon\).</p>

<div style="text-align:center">
  <img src="/assets/posts/2021-10-31-bias-variance-decomposition/01.png" style="width:90%; padding-top: 10px; padding-bottom: 10px;">
</div>

<p>It should be noted that the intuition presented here may not be applicable to all estimators out there, especially
if the model is a poor representation of the ground truth, no matter its complexity.
Additionally, there may be other effects at play that are less well understood. 
For instance, there is some empirical evidence in deep learning that heavily over-parameterized deep networks,
which are very flexible estimators, may still yield a meaningful bias-variance trade-off (e.g., <a href="#references">[1]</a>).</p>

<h2 id="references">References</h2>

<p>[1] Nakkiran et al. 2020 <em>Deep Double Descent: Where Bigger Models and More Data Hurt</em>, ICLR.</p>


  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    © Copyright 2024 Maren  Mahsereci.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
