<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Maren  Mahsereci


  | publications

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>Î©</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/publications/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://mmahsereci.github.io/">
       <span class="font-weight-bold">Maren</span>   Mahsereci
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              <!-- I changed this -->
              posts
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
    
      <img src="/assets/img/paper_thumbnails/Naslidnyk2023.png" class="bib">
    
  </div>

  <div id="Naslidnyk2023" class="col-sm-8">
    
      <div class="title">Comparing Scale Parameter Estimators for Gaussian Process Regression: Cross Validation and Maximum Likelihood</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Naslidnyk, M.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Kanagawa, M.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Karvonen, T.,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                and <em>Mahsereci, M.</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
      
        2023
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2307.07466" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/2307.07466.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Gaussian process (GP) regression is a Bayesian nonparametric method for regression and interpolation, offering a principled way of quantifying the uncertainties of predicted function values. For the quantified uncertainties to be well-calibrated, however, the covariance kernel of the GP prior has to be carefully selected. In this paper, we theoretically compare two methods for choosing the kernel in GP regression: cross-validation and maximum likelihood estimation. Focusing on the scale-parameter estimation of a Brownian motion kernel in the noiseless setting, we prove that cross-validation can yield asymptotically well-calibrated credible intervals for a broader class of ground-truth functions than maximum likelihood estimation, suggesting an advantage of the former over the latter.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Naslidnyk2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Comparing Scale Parameter Estimators for Gaussian Process Regression: Cross Validation and Maximum Likelihood}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Naslidnyk, M. and Kanagawa, M. and Karvonen, T. and Mahsereci, M.}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2307.07466}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2307.07466.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
    
      <img src="/assets/img/paper_thumbnails/Paleyes2023.png" class="bib">
    
  </div>

  <div id="Paleyes2023" class="col-sm-8">
    
      <div class="title">Emukit: A Python toolkit for decision making under uncertainty</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Paleyes, A.,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                <em>Mahsereci, M.</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  and Lawrence, N.D.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 22nd Python in Science Conference</em>
      
      
      
        2023
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://conference.scipy.org/proceedings/scipy2023/pdfs/emukit.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Emukit is a highly flexible Python toolkit for enriching decision making under uncertainty with statistical emulation. It is particularly pertinent to complex processes and simulations where data are scarce or difficult to acquire. Emukit provides a common framework for a range of iterative methods that propagate well-calibrated uncertainty estimates within a design loop, such as Bayesian optimisation, Bayesian quadrature and experimental design. It also provides multi-fidelity modelling capabilities. We describe the software design of the package, illustrate usage of the main APIs, and showcase the breadth of use cases in which the library already has been used by the research community.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Paleyes2023</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Paleyes, A. and Mahsereci, M. and Lawrence, N.D.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Emukit: A Python toolkit for decision making under uncertainty}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 22nd Python in Science Conference}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{68 - 75}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://conference.scipy.org/proceedings/scipy2023/pdfs/emukit.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
    
      <img src="/assets/img/paper_thumbnails/Wenger2021.png" class="bib">
    
  </div>

  <div id="Wenger2021" class="col-sm-8">
    
      <div class="title">ProbNum: Probabilistic Numerics in Python</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Wenger, J.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  KrÃ¤mer, N.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  PfÃ¶rtner, M.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Schmidt, J.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Bosch, N.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Effenberger, N.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Zenn, J.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Gessner, A.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Karvonen, T.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Briol, F-X,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                <em>Mahsereci, M.</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  and Hennig, P.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
      
        2021
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2112.02100" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/2112.02100.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Probabilistic numerical methods (PNMs) solve numerical problems via probabilistic inference. They have been developed for linear algebra, optimization, integration and differential equation simulation. PNMs naturally incorporate prior information about a problem and quantify uncertainty due to finite computational resources as well as stochastic input. In this paper, we present ProbNum: a Python library providing state-of-the-art probabilistic numerical solvers. ProbNum enables custom composition of PNMs for specific problem classes via a modular design as well as wrappers for off-the-shelf use. Tutorials, documentation, developer guides and benchmarks are available online at www.probnum.org.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Wenger2021</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ProbNum: Probabilistic Numerics in Python}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wenger, J. and KrÃ¤mer, N. and Pf{\"o}rtner, M. and Schmidt, J. and Bosch, N. and Effenberger, N. and Zenn, J. and Gessner, A. and Karvonen, T. and Briol, F-X and Mahsereci, M. and Hennig, P.}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2112.02100}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2112.02100.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
    
      <img src="/assets/img/paper_thumbnails/Naslidnyk2021.png" class="bib">
    
  </div>

  <div id="Naslidnyk2021" class="col-sm-8">
    
      <div class="title">Invariant Priors for Bayesian Quadrature</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Naslidnyk, M.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Gonzalez, J.,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                and <em>Mahsereci, M.</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Your Model is Wrong: Robustness and misspecification in probabilistic modeling Workshop, NeurIPS</em>
      
      
      
        2021
      
      
        <br><b>Accepted as contributed talk.</b>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/2112.01578.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Bayesian quadrature (BQ) is a model-based numerical integration method that is able to increase sample efficiency by encoding and leveraging known structure of the integration task at hand. In this paper, we explore priors that encode invariance of the integrand under a set of bijective transformations in the input domain, in particular some unitary transformations, such as rotations, axis-flips, or point symmetries. We show initial results on superior performance in comparison to standard Bayesian quadrature on several synthetic and one real world application.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Naslidnyk2021</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Invariant Priors for Bayesian Quadrature}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Naslidnyk, M. and Gonzalez, J. and Mahsereci, M.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Your Model is Wrong: Robustness and misspecification in probabilistic modeling Workshop, NeurIPS}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2112.01578.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
    
      <img src="/assets/img/paper_thumbnails/Siems2021.jpg" class="bib">
    
  </div>

  <div id="Siems2021" class="col-sm-8">
    
      <div class="title">Dynamic Pruning of a Neural Network via Gradient Signal-to-Noise Ratio</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Siems, J.N.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Klein, A.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Archambeau, C.,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                and <em>Mahsereci, M.</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 8th ICML Workshop on Automated Machine Learning (AutoML) </em>
      
      
      
        2021
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://openreview.net/pdf?id=34awaeWZgya" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
      
      <a href="https://slideslive.com/38962446/dynamic-pruning-of-a-neural-network-via-gradient-signaltonoise-ratio?ref=speaker-37497-latest" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a>
      
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>While training highly overparameterized neural networks is common practice in deep learning, research into post-hoc weight-pruning suggests that more than 90% of parameters can be removed without loss in predictive performance. To save resources, zero-shot and one-shot pruning attempt to find such a sparse representation at initialization or at an early stage of training. Though efficient, there is no justification, why the sparsity structure should not change during training. Dynamic sparsity pruning undoes this limitation and allows to adapt the structure of the sparse neural network during training. In this work we propose to use the gradient noise to make pruning decisions. The procedure enables us to automatically adjust the sparsity during training without imposing a hand-designed sparsity schedule, while at the same time being able to recover from previous pruning decisions by unpruning connections as necessary. We evaluate our method on image and tabular datasets and demonstrate that we reach similar performance as the dense model from which the sparse network is extracted, while exposing less hyperparameters than other dynamic sparsity methods.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Siems2021</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dynamic Pruning of a Neural Network via Gradient Signal-to-Noise Ratio}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Siems, J.N. and Klein, A. and Archambeau, C. and Mahsereci, M.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{8th ICML Workshop on Automated Machine Learning (AutoML) }</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://openreview.net/pdf?id=34awaeWZgya}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
    
      <img src="/assets/img/paper_thumbnails/Kersting2020.png" class="bib">
    
  </div>

  <div id="KerstingMahsereci2020" class="col-sm-8">
    
      <div class="title">A Fourier State Space Model for Bayesian ODE Filters</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Kersting, H.,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                and <em>Mahsereci, M.</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models, ICML</em>
      
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/2007.09118.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
      
      <a href="https://slideslive.com/38931446/a-fourier-state-space-model-for-bayesian-ode-filters" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a>
      
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Gaussian ODE filtering is a probabilistic numerical method to solve ordinary differential equations (ODEs). It computes a Bayesian posterior over the solution from evaluations of the vector field defining the ODE. Its most popular version, which employs an integrated Brownian motion prior, uses Taylor expansions of the mean to extrapolate forward and has the same convergence rates as classical numerical methods. As the solution of many important ODEs are periodic functions (oscillators), we raise the question whether Fourier expansions can also be brought to bear within the framework of Gaussian ODE filtering. To this end, we construct a Fourier state space model for ODEs and a âhybridâ model that combines a Taylor (Brownian motion) and Fourier state space model. We show by experiments how the hybrid model might become useful in cheaply predicting until the end of the time domain.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">KerstingMahsereci2020</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kersting, H. and Mahsereci, M.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A {Fourier} State Space Model for {Bayesian} {ODE} Filters}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models, ICML}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2007.09118.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
    
      <img src="/assets/img/paper_thumbnails/Gessner2019.png" class="bib">
    
  </div>

  <div id="Gessner2019" class="col-sm-8">
    
      <div class="title">Active Multi-Information Source Bayesian Quadrature</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Gessner, A.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Gonzalez, J.,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                and <em>Mahsereci, M.</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of The 35th Uncertainty in Artificial Intelligence Conference</em>
      
      
        22â25 jul
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="http://proceedings.mlr.press/v115/gessner20a.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Bayesian quadrature (BQ) is a sample-efficient probabilistic numerical method to solve integrals of expensive-to-evaluate black-box functions, yet so far, active BQ learning schemes focus merely on the integrand itself as information source, and do not allow for information transfer from cheaper, related functions. Here, we set the scene for active learning in BQ when multiple related information sources of variable cost (in input and source) are accessible. This setting arises for example when evaluating the integrand requires a complex simulation to be run that can be approximated by simulating at lower levels of sophistication and at lesser expense. We construct meaningful cost-sensitive multi-source acquisition-rates as an extension to common utility functions from vanilla BQ (VBQ), and discuss pitfalls that arise from blindly generalizing. In proof-of-concept experiments we scrutinize the behavior of our generalized acquisition functions. On an epidemiological model, we demonstrate that active multi-source BQ (AMS-BQ) is more cost-efficient than VBQ in learning the integral to a good accuracy. </p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Gessner2019</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Active Multi-Information Source Bayesian Quadrature}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gessner, A. and Gonzalez, J. and Mahsereci, M.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of The 35th Uncertainty in Artificial Intelligence Conference}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{712--721}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Adams, Ryan P. and Gogate, Vibhav}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{115}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="s">{22--25 Jul}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{http://proceedings.mlr.press/v115/gessner20a.}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li>
<div class="row">
  <div class="col-sm-2 abbr">
    
      <img src="/assets/img/paper_thumbnails/Paleyes2019.png" class="bib">
    
  </div>

  <div id="Paleyes2019" class="col-sm-8">
    
      <div class="title">Emulation of physical processes with Emukit</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Paleyes, A.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Pullin, M.,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                <em>Mahsereci, M.</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Lawrence, N.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Gonzalez, J.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Second Workshop on Machine Learning and the Physical Sciences, NeurIPS</em>
      
      
      
        2019
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://emukit.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      
      <a href="https://ml4physicalsciences.github.io/2019/files/NeurIPS_ML4PS_2019_113.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/EmuKit/emukit" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Decision making in uncertain scenarios is an ubiquitous challenge in real world systems. Tools to deal with this challenge include simulations to gather information and statistical emulation to quantify uncertainty. The machine learning community has developed a number of methods to facilitate decision making, but so far they are scattered in multiple different toolkits, and generally rely on a fixed backend. In this paper, we present Emukit, a highly adaptable Python toolkit for enriching decision making under uncertainty. Emukit allows users to: (i) use state of the art methods including Bayesian optimization, multi-fidelity emulation, experimental design, Bayesian quadrature and sensitivity analysis; (ii) easily prototype new decision making methods for new problems. Emukit is agnostic to the underlying modeling framework and enables users to use their own custom models. We show how Emukit can be used on three exemplary case studies.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Paleyes2019</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Paleyes, A. and Pullin, M. and Mahsereci, M. and Lawrence, N. and Gonzalez, J.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Emulation of physical processes with Emukit}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Second Workshop on Machine Learning and the Physical Sciences, NeurIPS}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://ml4physicalsciences.github.io/2019/files/NeurIPS_ML4PS_2019_113.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
    
      <img src="/assets/img/paper_thumbnails/Gessner2018.png" class="bib">
    
  </div>

  <div id="Gessner2018" class="col-sm-8">
    
      <div class="title">On Acquisition Functions for Active Multi-Source Bayesian Quadrature</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Gessner, A.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Gonzalez, J.,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                and <em>Mahsereci, M.</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In All of Bayesian Nonparametrics Workshop, NeurIPS</em>
      
      
      
        2018
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://d39w7f4ix9f5s9.cloudfront.net/aa/b2/50a316984beab1dd39244efc1f7d/scipub-272.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Bayesian quadrature (bq) is the method of choice for solving integrals of expensive-to-evaluate black-box functions. Active learning schemes so far only focus on the integrand itself, and do not allow for incorporation of cheaper, related information sources. Here, we set the scene for active learning in Bayesian quadrature when multiple related information sources of variable cost (in input and source) are accessible. In vanilla-bq (vbq) the same active learning scheme is induced by several measures of gain on the integral estimation. We show that this degeneracy is lifted in the multi-source setting, in which the previously interchangeably used vbq utilities yield different active learning schemes. In proof-of-concept experiments we scrutinize the behavior of our generalized acquisition functions.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Gessner2018</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gessner, A. and Gonzalez, J. and Mahsereci, M.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On Acquisition Functions for Active Multi-Source Bayesian Quadrature}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{All of Bayesian Nonparametrics Workshop, NeurIPS}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://d39w7f4ix9f5s9.cloudfront.net/aa/b2/50a316984beab1dd39244efc1f7d/scipub-272.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
    
      <img src="/assets/img/paper_thumbnails/Mahsereci2018.png" class="bib">
    
  </div>

  <div id="Mahsereci2018" class="col-sm-8">
    
      <div class="title">Probabilistic Approaches to Stochastic Optimization</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          

          
            
              <em>Mahsereci, M.</em>
            
          
        
      </div>

      <div class="periodical">
      
      
      
        2018
      
      
        <br><b>PhD thesis</b>
      
      </div>
    

    <div class="links">
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://publikationen.uni-tuebingen.de/xmlui/handle/10900/84726" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">Mahsereci2018</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Probabilistic Approaches to Stochastic Optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mahsereci, M.}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{Eberhard Karls Universit{\"a}t T{\"u}bingen, Germany}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{http://dx.doi.org/10.15496/publikation-26116}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
    
      <img src="/assets/img/paper_thumbnails/Mahsereci2017b.png" class="bib">
    
  </div>

  <div id="Mahsereci2017b" class="col-sm-8">
    
      <div class="title">Probabilistic Line Searches for Stochastic Optimization</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          

          
            
              
                <em>Mahsereci, M.</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  and Hennig, P.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Journal of Machine Learning Research</em>
      
      
      
        2017
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="http://jmlr.org/papers/v18/17-049.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      
      <a href="https://jmlr.csail.mit.edu/papers/volume18/17-049/17-049.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user- controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent. </p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Mahsereci2017b</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mahsereci, M. and Hennig, P.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Probabilistic Line Searches for Stochastic Optimization}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{18}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{119}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-59}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://jmlr.csail.mit.edu/papers/volume18/17-049/17-049.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
    
      <img src="/assets/img/paper_thumbnails/Mahsereci2017a.png" class="bib">
    
  </div>

  <div id="Mahsereci2017a" class="col-sm-8">
    
      <div class="title">Early Stopping Without a Validation Set</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          

          
            
              
                <em>Mahsereci, M.</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Balles, L.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Lassner, C.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Hennig, P.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
      
        2017
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1703.09580" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/1703.09580.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Early stopping is a widely used technique to prevent poor generalization performance when training an over-expressive model by means of gradient-based optimization. To find a good point to halt the optimizer, a common practice is to split the dataset into a training and a smaller validation set to obtain an ongoing estimate of the generalization performance. We propose a novel early stopping criterion based on fast-to-compute local statistics of the computed gradients and entirely removes the need for a held-out validation set. Our experiments show that this is a viable approach in the setting of least-squares and logistic regression, as well as neural networks.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Mahsereci2017a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Early Stopping Without a Validation Set}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mahsereci, M. and Balles, L. and Lassner, C. and Hennig, P.}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{1703.09580}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/1703.09580.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
    
      <img src="/assets/img/paper_thumbnails/Balles2017.png" class="bib">
    
  </div>

  <div id="Balles2017" class="col-sm-8">
    
      <div class="title">Automating Stochastic Optimization with Gradient Variance Estimates</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Balles, L.,
                
              
            
          
        
          
          
          
          
            
              
                
                
          

          
            
              
                <em>Mahsereci, M.</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  and Hennig, P.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>AutoML Workshop, ICML</em>
      
      
      
        2017
      
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://7bce9816-a-62cb3a1a-s-sites.googlegroups.com/site/automl2017icml/accepted-papers/AutoML_2017_paper_6.pdf?attachauth=ANoY7cq6kv_KSergf07tEvRAgtMZxioI5VJzpW0RCbX73jdWHIJi9UVTpkLufRzx8cgpYUi3rrrvT6gNHohQ7dHZv6duOYUjrPSNSDK_EpskgjtOxAA4nlEY48Sy2v_QsfEaFatZmdXfP-M43RzAWEE4rB8qq5sE-Q0mfA89auEEeHknxHbcsqPjX_zi8bDI_oEs5XpzDZsUpw8tK9FvKsPRixHMUEaQR3Yvj1NVk9yWcPua7gzBIcsebE8DGhnRhDZ-NC2-4Onu&amp;attredirects=0" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>One reason optimization methods might have to expose free parameters to the user, rather than to set them internally, is that these parameters are not identifiable given the âobservationsâ available to the method. We discuss the use of gradient variance estimates as an additional source of information allowing for automation of stochastic optimization algorithms. We review several recent results that use such estimates to eliminate (i.e., determine internally, without user intervention) hyper-parameters of stochastic optimizers and contribute a detailed discussion of the efficient implementation of gradient variance estimates for neural networks. </p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Balles2017</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Balles, L. and Mahsereci, M. and Hennig, P.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automating Stochastic Optimization with Gradient Variance Estimates}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{AutoML Workshop, ICML}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://7bce9816-a-62cb3a1a-s-sites.googlegroups.com/site/automl2017icml/accepted-papers/AutoML_2017_paper_6.pdf?attachauth=ANoY7cq6kv_KSergf07tEvRAgtMZxioI5VJzpW0RCbX73jdWHIJi9UVTpkLufRzx8cgpYUi3rrrvT6gNHohQ7dHZv6duOYUjrPSNSDK_EpskgjtOxAA4nlEY48Sy2v_QsfEaFatZmdXfP-M43RzAWEE4rB8qq5sE-Q0mfA89auEEeHknxHbcsqPjX_zi8bDI_oEs5XpzDZsUpw8tK9FvKsPRixHMUEaQR3Yvj1NVk9yWcPua7gzBIcsebE8DGhnRhDZ-NC2-4Onu&amp;attredirects=0}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography"><li>
<div class="row">
  <div class="col-sm-2 abbr">
    
      <img src="/assets/img/paper_thumbnails/Mahsereci2015.png" class="bib">
    
  </div>

  <div id="Mahsereci2015" class="col-sm-8">
    
      <div class="title">Probabilistic Line Searches for Stochastic Optimization</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          

          
            
              
                <em>Mahsereci, M.</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  and Hennig, P.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems 28</em>
      
      
      
        2015
      
      
        <br><b>Selected as full oral (&lt; 1% acceptance).</b>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="http://papers.nips.cc/paper/5753-probabilistic-line-searches-for-stochastic-optimization.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Mahsereci2015</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Probabilistic Line Searches for Stochastic Optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mahsereci, M. and Hennig, P.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems 28}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{181--189}</span><span class="p">,</span>
  <span class="na">editors</span> <span class="p">=</span> <span class="s">{C. Cortes, N.D. Lawrence, D.D. Lee, M. Sugiyama and R. Garnett}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Curran Associates, Inc.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{http://papers.nips.cc/paper/5753-probabilistic-line-searches-for-stochastic-optimization.pdf}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    Â© Copyright 2024 Maren  Mahsereci.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
